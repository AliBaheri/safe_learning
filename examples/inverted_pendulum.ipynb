{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import GPflow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from scipy import linalg\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x\n",
    "\n",
    "import safe_learning\n",
    "import plotting\n",
    "\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "\n",
    "Optimize over the policy such that the safe set does not shrink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 2\n",
    "m = 1\n",
    "\n",
    "# 'Wrong' model parameters\n",
    "mass = 0.1\n",
    "friction = 0.\n",
    "length = 0.5\n",
    "gravity = 9.81\n",
    "inertia = mass * length ** 2\n",
    "\n",
    "# True model parameters\n",
    "true_mass = 0.15\n",
    "true_friction = 0.05\n",
    "true_length = length\n",
    "true_inertia = true_mass * true_length ** 2\n",
    "\n",
    "# Input saturation\n",
    "x_max = np.deg2rad(30)\n",
    "u_max = gravity * true_mass * true_length * np.sin(x_max)\n",
    "\n",
    "norm_state = np.array([x_max, np.sqrt(gravity / length)])\n",
    "norm_action = np.array([u_max])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "class InvertedPendulum(safe_learning.DeterministicFunction):\n",
    "    \"\"\"Inverted Pendulum.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mass : float\n",
    "    length : float\n",
    "    friction : float, optional\n",
    "    dt : float, optional\n",
    "        The sampling time.\n",
    "    normalization : tuple, optional\n",
    "        A tuple (Tx, Tu) of arrays used to normalize the state and actions. It\n",
    "        is so that diag(Tx) *x_norm = x and diag(Tu) * u_norm = u.\n",
    "    \"\"\"\n",
    "    def __init__(self, mass, length, friction=0, dt=1/80, normalization=None):\n",
    "        super(InvertedPendulum, self).__init__()\n",
    "        self.mass = mass\n",
    "        self.length = length\n",
    "        self.gravity = 9.81\n",
    "        self.friction = friction\n",
    "        self.dt = dt\n",
    "        \n",
    "        self.normalization = normalization\n",
    "        if normalization is not None:\n",
    "            self.normalization = [arr.astype(safe_learning.config.np_dtype) for arr in normalization]\n",
    "            self.inv_norm = [arr ** -1 for arr in self.normalization]\n",
    "            \n",
    "    @property\n",
    "    def inertia(self):\n",
    "        \"\"\"The inertia of the pendulum\"\"\"\n",
    "        return self.mass * self.length ** 2\n",
    "    \n",
    "    def normalize(self, state, action):\n",
    "        \"\"\"Normalize states and actions.\"\"\"\n",
    "        if self.normalization is None:\n",
    "            return state, action\n",
    "        \n",
    "        Tx_inv, Tu_inv = map(np.diag, self.inv_norm)\n",
    "        state = tf.matmul(state, Tx_inv)\n",
    "        \n",
    "        if action is not None:\n",
    "            action = tf.matmul(action, Tu)\n",
    "            \n",
    "        return state, action\n",
    "    \n",
    "    def denormalize(self, state, action):\n",
    "        \"\"\"Denormalize states and actions\"\"\"\n",
    "        if self.normalization is None:\n",
    "            return state, action\n",
    "        \n",
    "        Tx, Tu = map(np.diag, self.normalization)\n",
    "        \n",
    "        state = tf.matmul(state, Tx)\n",
    "        if action is not None:\n",
    "            action = tf.matmul(action, Tu)\n",
    "        \n",
    "        return state, action\n",
    "    \n",
    "    def linearize(self):\n",
    "        \"\"\"Return the linearized system.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        a : ndarray\n",
    "            The state matrix.\n",
    "        b : ndarray\n",
    "            The action matrix.\n",
    "        \"\"\"\n",
    "        gravity = self.gravity\n",
    "        length = self.length\n",
    "        friction = self.friction\n",
    "        inertia = self.inertia\n",
    "        \n",
    "        A = np.array([[0, 1],\n",
    "                      [gravity / length, -friction / inertia]],\n",
    "                    dtype=safe_learning.config.np_dtype)\n",
    "        \n",
    "        B = np.array([[0],\n",
    "                      [1 / inertia]],\n",
    "                    dtype=safe_learning.config.np_dtype)\n",
    "        \n",
    "        if self.normalization is not None:\n",
    "            Tx, Tu = map(np.diag, self.normalization)\n",
    "            Tx_inv, Tu_inv = map(np.diag, self.inv_norm)\n",
    "            \n",
    "            A = np.linalg.multi_dot((Tx_inv, A, Tx))\n",
    "            B = np.linalg.multi_dot((Tx_inv, B, Tu))\n",
    "            \n",
    "        sys = signal.StateSpace(A, B, np.eye(2), np.zeros((2, 1)))\n",
    "        sysd = sys.to_discrete(self.dt)\n",
    "        return sysd.A, sysd.B\n",
    "    \n",
    "    @safe_learning.utilities.concatenate_inputs(start=1)\n",
    "    def evaluate(self, state_action):\n",
    "        \"\"\"Evaluate the dynamics\"\"\"\n",
    "        # Denormalize\n",
    "        state, action = tf.split(state_action, [2,  1], axis=1)\n",
    "        state, action = self.denormalize(state, action)\n",
    "        \n",
    "        n_inner = 10\n",
    "        dt = self.dt / n_inner\n",
    "        for i in range(n_inner):\n",
    "            state_derivative = self.ode(state, action)\n",
    "            state = state + dt * state_derivative\n",
    "            \n",
    "        return self.normalize(state, None)[0]\n",
    "        \n",
    "    def ode(self, state, action):\n",
    "        \"\"\"Compute the state time-derivative.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        states: ndarray or Tensor\n",
    "            Unnormalized states.\n",
    "        actions: ndarray or Tensor\n",
    "            Unnormalized actions.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        x_dot: Tensor\n",
    "            The normalized derivative of the dynamics\n",
    "        \"\"\"\n",
    "\n",
    "        # Physical dynamics\n",
    "        gravity = self.gravity\n",
    "        length = self.length\n",
    "        friction = self.friction\n",
    "        inertia = self.inertia\n",
    "        \n",
    "        angle, angular_velocity = tf.split(state, 2, axis=1)\n",
    "        \n",
    "        x_ddot = gravity / length * tf.sin(angle) + action / inertia\n",
    "        \n",
    "        if friction > 0:\n",
    "            x_ddot -= friction / inertia * angular_velocity\n",
    "        \n",
    "        state_derivative = tf.concat((angular_velocity, x_ddot), axis=1)\n",
    "\n",
    "        # Normalize\n",
    "        return state_derivative #self.normalize(state_derivative, None)[0]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_dynamics = InvertedPendulum(mass=true_mass, length=true_length, friction=true_friction,\n",
    "                                normalization=(norm_state, norm_action))\n",
    "\n",
    "wrong_pendulum = InvertedPendulum(mass=mass, length=length, friction=friction,\n",
    "                                  normalization=(norm_state, norm_action))\n",
    "\n",
    "# LQR cost matrices\n",
    "q = 1 * np.diag([1., 2.])\n",
    "r = 1.2 * np.array([[1]], dtype=safe_learning.config.np_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_min, x_max, discretization\\\n",
    "state_limits = np.array([[-1, 1], [-1., 1.]])\n",
    "action_limits = np.array([[-1, 1]])\n",
    "num_states = [2001, 2001]\n",
    "num_actions = [101]\n",
    "\n",
    "discretization = safe_learning.GridWorld(np.vstack((state_limits, action_limits)),\n",
    "                                         num_states + num_actions)\n",
    "\n",
    "safety_disc = safe_learning.GridWorld(state_limits, num_states)\n",
    "action_disc = safe_learning.GridWorld(action_limits, num_actions)\n",
    "policy_disc = safe_learning.GridWorld(state_limits, [55, 55])\n",
    "\n",
    "# Discretization constant\n",
    "tau = np.min(safety_disc.unit_maxes)\n",
    "\n",
    "print('Grid size: {0}'.format(safety_disc.nindex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GP dynamics model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = wrong_pendulum.linearize()\n",
    "lipschitz_dynamics = 1\n",
    "\n",
    "noise_var = 0.001 ** 2\n",
    "\n",
    "m_true = np.hstack((true_dynamics.linearize()))\n",
    "m = np.hstack((A, B))\n",
    "\n",
    "variances = (m_true - m) ** 2\n",
    "\n",
    "# Make sure things remain \n",
    "np.clip(variances, 1e-5, None, out=variances)\n",
    "\n",
    "# # Kernels\n",
    "# kernel1 = (GPflow.kernels.Matern32(3, lengthscales=np.sqrt(3)) *\n",
    "#            GPflow.kernels.Linear(3, variance=variances[0, :], ARD=True))\n",
    "# kernel2 = (GPflow.kernels.Matern32(3, lengthscales=np.sqrt(3)) *\n",
    "#            GPflow.kernels.Linear(3, variance=variances[1, :], ARD=True))\n",
    "\n",
    "kernel1 = (GPflow.kernels.Linear(3, variance=variances[0, :], ARD=True)\n",
    "           + GPflow.kernels.Matern32(1, lengthscales=1, active_dims=[0])\n",
    "           * GPflow.kernels.Linear(1, variance=variances[0, 1]))\n",
    "\n",
    "kernel2 = (GPflow.kernels.Linear(3, variance=variances[1, :], ARD=True)\n",
    "           + GPflow.kernels.Matern32(1, lengthscales=1, active_dims=[0])\n",
    "           * GPflow.kernels.Linear(1, variance=variances[1, 1]))\n",
    "\n",
    "# Mean dynamics\n",
    "\n",
    "mean_dynamics = safe_learning.LinearSystem((A, B), name='mean_dynamics')\n",
    "mean_function1 = safe_learning.LinearSystem((A[[0], :], B[[0], :]), name='mean_dynamics_1')\n",
    "mean_function2 = safe_learning.LinearSystem((A[[1], :], B[[1], :]), name='mean_dynamics_2')\n",
    "\n",
    "# Define a GP model over the dynamics\n",
    "gp1 = GPflow.gpr.GPR(np.empty((0, 3), dtype=safe_learning.config.np_dtype),\n",
    "                    np.empty((0, 1), dtype=safe_learning.config.np_dtype),\n",
    "                    kernel1,\n",
    "                    mean_function=mean_function1)\n",
    "gp1.likelihood.variance = noise_var\n",
    "\n",
    "gp2 = GPflow.gpr.GPR(np.empty((0, 3), dtype=safe_learning.config.np_dtype),\n",
    "                    np.empty((0, 1), dtype=safe_learning.config.np_dtype),\n",
    "                    kernel2,\n",
    "                    mean_function=mean_function2)\n",
    "gp2.likelihood.variance = noise_var\n",
    "\n",
    "gp1_fun = safe_learning.GaussianProcess(gp1)\n",
    "gp2_fun = safe_learning.GaussianProcess(gp2)\n",
    "\n",
    "dynamics = safe_learning.FunctionStack((gp1_fun, gp2_fun))\n",
    "\n",
    "\n",
    "# Get the optimal control gain\n",
    "a_true, b_true = true_dynamics.linearize()\n",
    "k_opt, s_opt = safe_learning.utilities.dlqr(a_true, b_true, q, r)\n",
    "optimal_policy = safe_learning.LinearSystem((-k_opt))\n",
    "optimal_policy = safe_learning.Saturation(optimal_policy, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# q = 1 * np.diag([1., 2.])\n",
    "# r = 1.2 * np.array([[1]], dtype=safe_learning.config.np_dtype)\n",
    "\n",
    "# # a_true, b_true = true_dynamics.linearize()\n",
    "# k_opt, s_opt = safe_learning.utilities.dlqr(a_true, b_true, q, r)\n",
    "\n",
    "# value_function = safe_learning.QuadraticFunction(-s_opt)\n",
    "\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# states = policy_disc.all_points\n",
    "\n",
    "# values = value_function(states).eval()\n",
    "# values[np.abs(values) >= 50] = 50\n",
    "\n",
    "# img = plt.imshow(values.reshape(policy_disc.num_points).T,\n",
    "#                  origin='lower',\n",
    "#                  extent=rl.value_function.discretization.limits.ravel())\n",
    "# plt.colorbar(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning for the mean dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initial policy\n",
    "k, s = safe_learning.utilities.dlqr(A, B, q, r)\n",
    "init_policy = np.clip(policy_disc.all_points.dot(-k.T),\n",
    "                      action_limits[:, 0],\n",
    "                      action_limits[:, 1])\n",
    "policy = safe_learning.Triangulation(policy_disc, init_policy)\n",
    "\n",
    "\n",
    "gamma = 0.98\n",
    "value_function = safe_learning.Triangulation(policy_disc, np.zeros_like(init_policy), project=True)\n",
    "\n",
    "\n",
    "reward_function = safe_learning.QuadraticFunction(linalg.block_diag(-q, -r))\n",
    "\n",
    "\n",
    "rl = safe_learning.PolicyIteration(\n",
    "    policy,\n",
    "    dynamics,\n",
    "    reward_function,\n",
    "    value_function,\n",
    "    gamma=gamma)\n",
    "\n",
    "with tf.variable_scope('rl_mean_optimization'):\n",
    "    rl_opt_value_function = rl.optimize_value_function()\n",
    "#     rl_opt_value_function = rl.value_iteration()\n",
    "\n",
    "    policy_loss = -tf.reduce_mean(rl.future_values(rl.state_space))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    \n",
    "    adapt_policy = optimizer.minimize(policy_loss, var_list=[rl.policy.parameters])\n",
    "    \n",
    "#     adapt_policy = safe_learning.utilities.gradient_clipping(optimizer,\n",
    "#                                                              policy_loss,\n",
    "#                                                              var_list=[rl.policy.parameters],\n",
    "#                                                              limits=[(-1., 1.)])\n",
    "\n",
    "    # Clip the policy values to comply with the actuation limits\n",
    "    adapt_policy = safe_learning.utilities.add_constraint(adapt_policy,\n",
    "                                                          var_list=[rl.policy.parameters],\n",
    "                                                          bound_list=action_limits)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = action_disc.all_points\n",
    "\n",
    "def rl_optimize_policy():\n",
    "    rl_opt_value_function.eval(feed_dict=rl.feed_dict)\n",
    "    for i in range(100):\n",
    "        adapt_policy.eval(feed_dict=rl.feed_dict)\n",
    "#     rl.discrete_policy_optimization(action_space)\n",
    "    \n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    rl_optimize_policy()\n",
    "# rl_opt_value_function.eval(feed_dict=rl.feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy():\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    img = plt.imshow(rl.policy.parameters.eval().reshape(policy_disc.num_points).T,\n",
    "                     origin='lower',\n",
    "                     extent=rl.policy.discretization.limits.ravel())\n",
    "    plt.colorbar(img)\n",
    "    \n",
    "plot_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_values():\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    img = plt.imshow(rl.value_function.parameters.eval().reshape(policy_disc.num_points).T,\n",
    "                     origin='lower',\n",
    "                     extent=rl.value_function.discretization.limits.ravel())\n",
    "    plt.colorbar(img)\n",
    "plot_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Lyapunov function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lyapunov_function = safe_learning.QuadraticFunction(s_opt)\n",
    "# lipschitz_lyapunov = lambda x: tf.reduce_max(tf.abs(tf.matmul(x, s_opt + s_opt.T)),\n",
    "#                                              axis=1, keep_dims=True)\n",
    "\n",
    "lyapunov_function = -rl.value_function\n",
    "lipschitz_lyapunov = lambda x: tf.reduce_max(tf.abs(rl.value_function.gradient(x)),\n",
    "                                             axis=1, keep_dims=True)\n",
    "\n",
    "lipschitz_dynamics = np.max(m_true)\n",
    "\n",
    "lyapunov = safe_learning.Lyapunov(safety_disc,\n",
    "                                  lyapunov_function,\n",
    "                                  dynamics,\n",
    "                                  lipschitz_dynamics,\n",
    "                                  lipschitz_lyapunov,\n",
    "                                  tau,\n",
    "                                  policy=rl.policy,\n",
    "                                  initial_set=None)\n",
    "\n",
    "true_lyapunov = safe_learning.Lyapunov(safety_disc,\n",
    "                                       lyapunov_function,\n",
    "                                       true_dynamics,\n",
    "                                       lipschitz_dynamics,\n",
    "                                       lipschitz_lyapunov,\n",
    "                                       tau,\n",
    "                                       policy=optimal_policy,\n",
    "                                       initial_set=None)\n",
    "\n",
    "true_lyapunov_init = safe_learning.Lyapunov(safety_disc,\n",
    "                                       lyapunov_function,\n",
    "                                       true_dynamics,\n",
    "                                       lipschitz_dynamics,\n",
    "                                       lipschitz_lyapunov,\n",
    "                                       tau,\n",
    "                                       policy=policy,\n",
    "                                       initial_set=None)\n",
    "\n",
    "# Set initial safe set (level set)\n",
    "values = safe_learning.QuadraticFunction(s_opt)(safety_disc.all_points).eval()\n",
    "cutoff = np.max(values) * 0.005\n",
    "\n",
    "\n",
    "lyapunov.initial_safe_set = np.squeeze(values, axis=1) <= cutoff\n",
    "\n",
    "true_lyapunov.initial_safe_set = lyapunov.initial_safe_set\n",
    "true_lyapunov_init.initial_safe_set = lyapunov.initial_safe_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_things(lyapunov):\n",
    "    lyapunov.update_safe_set()\n",
    "#     lyapunov.safe_set[lyapunov.initial_safe_set] = True\n",
    "    \n",
    "    plt.imshow(lyapunov.safe_set.reshape(num_states).T,\n",
    "               origin='lower',\n",
    "               extent=lyapunov.discretization.limits.ravel(),\n",
    "               vmin=0,\n",
    "               vmax=1)\n",
    "    \n",
    "    if isinstance(lyapunov.dynamics, safe_learning.UncertainFunction):\n",
    "        X = lyapunov.dynamics.functions[0].X\n",
    "        plt.plot(X[:, 0], X[:, 1], 'rx')\n",
    "    \n",
    "    plt.title('safe set')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "# plot_things(lyapunov)\n",
    "plot_things(lyapunov)\n",
    "# plot_things(true_lyapunov)\n",
    "# plot_things(true_lyapunov_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online learning\n",
    "As we sample within this initial safe set, we gain more knowledge about the system. In particular, we iteratively select the state withing the safe set, $\\mathcal{S}_n$, where the dynamics are the most uncertain (highest variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('opt2'):\n",
    "    \n",
    "    # Placeholder for states\n",
    "    tf_states = tf.placeholder(safe_learning.config.dtype, [None, 2])\n",
    "    \n",
    "    # Add Lyapunov uncertainty (but only if safety-relevant)\n",
    "    values = rl.future_values(tf_states, lyapunov=lyapunov)\n",
    "    \n",
    "    policy_loss = -tf.reduce_mean(values)\n",
    "    \n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    adapt_policy = optimizer.minimize(policy_loss, var_list=[rl.policy.parameters])\n",
    "    \n",
    "    # Clip the policy values to comply with the actuation limits\n",
    "    adapt_policy = safe_learning.utilities.add_constraint(adapt_policy,\n",
    "                                                          var_list=[rl.policy.parameters],\n",
    "                                                          bound_list=action_limits)[0]\n",
    "    \n",
    "    \n",
    "def rl_optimize_policy(num_iter):\n",
    "    # Optimize value function\n",
    "    rl_opt_value_function.eval(feed_dict=rl.feed_dict)\n",
    "    all_states = lyapunov.discretization.all_points\n",
    "\n",
    "    # select random training batches\n",
    "    for i in tqdm(range(num_iter)):\n",
    "        idx = np.random.choice(len(all_states), size=1000, replace=False)\n",
    "        rl.feed_dict[tf_states] = all_states[idx]\n",
    "\n",
    "        adapt_policy.eval(feed_dict=rl.feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_variation = np.array([[-0.02], [0.], [0.02]], dtype=safe_learning.config.np_dtype)\n",
    "\n",
    "\n",
    "with tf.variable_scope('add_new_measurement'):\n",
    "        action_dim = lyapunov.policy.output_dim\n",
    "        tf_max_state_action = tf.placeholder(safe_learning.config.dtype,\n",
    "                                             shape=[1, safety_disc.ndim + action_dim])\n",
    "        tf_measurement = true_dynamics(tf_max_state_action)\n",
    "        \n",
    "def update_gp():\n",
    "    \"\"\"Update the GP model based on an actively selected data point.\"\"\"\n",
    "    # Get a new sample location\n",
    "    max_state_action, _ = safe_learning.get_safe_sample(lyapunov,\n",
    "                                                        action_variation,\n",
    "                                                        action_limits,\n",
    "                                                        num_samples=1000)\n",
    "\n",
    "    # Obtain a measurement of the true dynamics\n",
    "    lyapunov.feed_dict[tf_max_state_action] = max_state_action\n",
    "    measurement = tf_measurement.eval(feed_dict=lyapunov.feed_dict)\n",
    "\n",
    "    # Add the measurement to our GP dynamics\n",
    "    lyapunov.dynamics.add_data_point(max_state_action, measurement)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lyapunov.update_safe_set()\n",
    "plot_things(lyapunov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i)\n",
    "    for i in tqdm(range(10)):\n",
    "        update_gp()\n",
    "    \n",
    "    rl_optimize_policy(num_iter=200)\n",
    "    lyapunov.update_values()\n",
    "    lyapunov.update_safe_set()\n",
    "    \n",
    "    # Plotting includes an update to the levelset!\n",
    "    plot_things(lyapunov)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
