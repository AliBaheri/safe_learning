{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy\n",
    "%matplotlib inline\n",
    "\n",
    "# Try to import safe_rl from system\n",
    "# if it fails get it from the main folder directly instead.\n",
    "try:\n",
    "    import safe_rl\n",
    "except ImportError:\n",
    "    import utilities\n",
    "    safe_rl = utilities.import_from_directory('safe_rl', '../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get matrix for values of test points\n",
    "\n",
    "For each vertex x, the next state is given by some function $f(x)$. The goal is to find the value $V(f(x))$ as a function of the values at the other vertices. This value can then be used as a constraint in the optimization problem.\n",
    "$$V(f(x)) = a^\\mathrm{T} V(\\mathrm{vertices})$$\n",
    "\n",
    "We start by finding the simplex corresponding to $f(x)$. Given this simplex's vertices, we solve for $a$ above, using one arbitrary vertex as a reference point:\n",
    "$$V(f(x)) = V(v_1) + a_1 V(v_2) + a_2 V(v_3)$$\n",
    "\n",
    "The hyperplane spanned by the simplex is given by\n",
    "\\begin{align*}\n",
    "\\xi_1 ( x_2 - x_1) + \\xi_2 (y_2 - y_1) = V(v_2) - V(v_1) \\\\\n",
    "\\xi_1 ( x_3 - x_1) + \\xi_2 (y_3 - y_1) = V(v_3) - V(v_1) \n",
    "\\end{align*}\n",
    "\n",
    "and, as a result,\n",
    "$$V(f(x_t)) = V(v_1) + (x_t - x_1, y_t - y_1)\n",
    "\\left( \\begin{matrix}\n",
    "x_2 - x_1 & y_2 - y_1 \\\\\n",
    "x_3 - x_1 & y_3 - y_1\n",
    "\\end{matrix} \\right)^{-1}\n",
    "\\left( \\begin{matrix}\n",
    "V(v_2) - V(v_1) \\\\\n",
    "V(v_3) - V(v_1)\n",
    "\\end{matrix} \\right),\n",
    "$$\n",
    "which can easily be solved for $a_1$ and $a_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Add fake terminal state with zero reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dynamics(states, actions):\n",
    "    \"\"\"Return future states of the car\"\"\"\n",
    "    states = np.atleast_2d(states)\n",
    "    actions = np.atleast_2d(actions)\n",
    "    future_states = states.copy()\n",
    "    future_states[:, 0] += states[:, 1]\n",
    "    future_states[:, 1] += 0.001 * actions[:, 0] - 0.0025 * np.cos(3 * states[:, 0])\n",
    "    \n",
    "    # Add constraints\n",
    "    np.clip(future_states[:, 1], -0.07, 0.07, out=future_states[:, 1])\n",
    "    future_states[states[:, 0] >= 0.6, 0] = 0.6\n",
    "    future_states[future_states[:, 0] < -1.2, 0] = -1.2\n",
    "    return future_states\n",
    "\n",
    "def reward_function(states, actions, next_states):\n",
    "    states = np.atleast_2d(states)\n",
    "#     return np.logical_and(states[:, 0] < 0.6, next_states[:, 0] >= 0.6).astype('int')\n",
    "    return -0.01 * (next_states[:, 0] < 0.6).astype('int')\n",
    "\n",
    "domain = [[-1.3, 0.7], [-.08, .08]]\n",
    "n_points = [5, 5]\n",
    "delaunay = safe_rl.Delaunay(domain, n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states = delaunay.index_to_state(np.arange(delaunay.nindex))\n",
    "\n",
    "# random actions\n",
    "action_space = np.array([-1, 1])\n",
    "policy = np.random.rand(len(states)) > 0.5\n",
    "\n",
    "def get_value_function(policy):\n",
    "    next_states = dynamics(states, policy)\n",
    "    rewards = reward_function(states, policy, next_states)\n",
    "\n",
    "    h = delaunay.function_values_at(next_states).toarray()\n",
    "    values = cvxpy.Variable(delaunay.nindex)\n",
    "    eps = cvxpy.Variable(delaunay.nindex)\n",
    "    future_values = h * values\n",
    "\n",
    "    gamma = cvxpy.Parameter(sign='positive')\n",
    "\n",
    "    objective = cvxpy.Maximize(cvxpy.sum_entries(values))\n",
    "    constraints = [values <= rewards + gamma * future_values]\n",
    "\n",
    "    prob = cvxpy.Problem(objective, constraints)\n",
    "    gamma.value = 0.99\n",
    "    prob.solve()\n",
    "    \n",
    "    return np.asarray(values.value).squeeze(), prob, np.asarray(eps.value).squeeze()\n",
    "\n",
    "\n",
    "\n",
    "def optimize_policy(vertex_values):\n",
    "    values = np.empty((len(vertex_values), len(action_space)), dtype=np.float)\n",
    "    for i, action in enumerate(action_space):\n",
    "        next_states = dynamics(states, action * np.ones((len(states), 1)))\n",
    "        values[:, i] = delaunay.function_values_at(next_states, vertex_values=vertex_values)\n",
    "        \n",
    "    best_actions = np.argmax(values, axis=1)\n",
    "    return action_space[best_actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_values = -100 * np.ones(delaunay.nindex)\n",
    "old_policy = np.zeros_like(policy)\n",
    "\n",
    "for i in range(100):\n",
    "    values, prob, error = get_value_function(policy)\n",
    "    \n",
    "    if not prob.status == cvxpy.OPTIMAL:\n",
    "        print('{} - optimizeation status: {}'.format(i, prob.status))\n",
    "        break\n",
    "        \n",
    "    policy = optimize_policy(values)\n",
    "    \n",
    "    if np.max(np.abs(old_values - values)) <= 1e-10 and np.all(old_policy == policy):\n",
    "        print('converged after {} iterations'.format(i))\n",
    "        break\n",
    "    else:\n",
    "        old_values = values.copy()\n",
    "        old_policy = policy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vals = values.reshape(n_points[0] + 1, n_points[1] + 1).T[::-1]\n",
    "\n",
    "ax = plt.imshow(vals, origin='upper',\n",
    "                extent=domain[0] + domain[1],\n",
    "                aspect='auto', cmap='viridis')\n",
    "plt.colorbar(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acts = policy.reshape(n_points[0] + 1, n_points[1] + 1).T[::-1]\n",
    "ax = plt.imshow(acts, origin='upper', extent=domain[0] + domain[1], aspect='auto')\n",
    "plt.colorbar(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states = np.zeros((1000, 2), dtype=np.float)\n",
    "states[0, 0] = -0.5\n",
    "\n",
    "for i in range(len(states) - 1):\n",
    "    action = delaunay.function_values_at(states[i, :], vertex_values=policy)\n",
    "#     index = delaunay.state_to_index(states[i, :])\n",
    "#     action = policy[index]\n",
    "    states[i+1, :] = dynamics(states[i, :], action)\n",
    "    if states[i+1, 0] >= 0.6:\n",
    "        states[i+1:, :] = states[i+1]\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(states[:,0], states[:, 1])\n",
    "plt.xlabel('pos')\n",
    "plt.ylabel('vel')\n",
    "plt.xlim(-1.2, 0.6)\n",
    "plt.ylim(-0.07, 0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
