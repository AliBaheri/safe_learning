{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import cvxpy\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "# Try to import safe_rl from system\n",
    "# if it fails get it from the main folder directly instead.\n",
    "try:\n",
    "    import safe_rl\n",
    "except ImportError:\n",
    "    import utilities\n",
    "    safe_rl = utilities.import_from_directory('safe_rl', '../')\n",
    "    \n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_triangulation(values, axis=None, three_dimensional=False, **kwargs):\n",
    "    \"\"\"Plot a triangulation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    values: ndarray\n",
    "    axis: optional\n",
    "    three_dimensional: bool, optional\n",
    "        Whether to plot 3D\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    axis:\n",
    "        The axis on which we plotted.\n",
    "    \"\"\"\n",
    "    if three_dimensional:\n",
    "        if axis is None:\n",
    "            axis = Axes3D(plt.figure())\n",
    "\n",
    "        # Get the simplices and plot\n",
    "        simplices = delaunay.simplices(np.arange(delaunay.nsimplex))\n",
    "        c = axis.plot_trisurf(state_space[:, 0], state_space[:, 1], values,\n",
    "                            triangles=simplices.copy(),\n",
    "                            cmap='viridis', lw=0.1, **kwargs)\n",
    "        plt.colorbar(c)\n",
    "    else:\n",
    "        if axis is None:\n",
    "            axis = plt.figure().gca()\n",
    "            \n",
    "        # Some magic reshaping to go to physical coordinates\n",
    "        vals = values.reshape(n_points[0] + 1, n_points[1] + 1).T[::-1]\n",
    "        axis = plt.imshow(vals.copy(), origin='upper',\n",
    "                        extent=domain[0] + domain[1],\n",
    "                        aspect='auto', cmap='viridis', interpolation='bilinear', **kwargs)\n",
    "        plt.colorbar(axis)\n",
    "        axis = axis.axes\n",
    "        \n",
    "    return axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Handle different terminal states in a better way!\n",
    "## TODO: Why are there troubles for finely discretized domains and many iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domain = [[-1.2, 0.7], [-.07, .07]]\n",
    "n_points = [10, 10]\n",
    "delaunay = safe_rl.Delaunay(domain, n_points)\n",
    "\n",
    "state_space = delaunay.index_to_state(np.arange(delaunay.nindex))\n",
    "action_space = np.array([-1, 1])\n",
    "\n",
    "gamma = 0.99\n",
    "terminal_reward = 1\n",
    "\n",
    "\n",
    "def dynamics(states, actions):\n",
    "    \"\"\"Return future states of the car\"\"\"\n",
    "    states = np.atleast_2d(states)\n",
    "    actions = np.atleast_2d(actions)\n",
    "    \n",
    "    next_states = states.copy()\n",
    "    \n",
    "    next_states[:, 0] += states[:, 1]\n",
    "    next_states[:, 1] += 0.001 * actions[:, 0] - 0.0025 * np.cos(3 * states[:, 0])\n",
    "    \n",
    "    return next_states\n",
    "\n",
    "\n",
    "def reward_function(states, actions, next_states):\n",
    "    states = np.atleast_2d(states)\n",
    "    return terminal_reward * (states[:, 0] >= 0.6).astype('int')\n",
    "\n",
    "\n",
    "def is_terminal(states):\n",
    "    \"\"\"Return true if states are terminal.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    states: ndarray\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    is_temrminal: boolean array\n",
    "    \"\"\"\n",
    "    return states[:, 0] >= 0.6\n",
    "\n",
    "\n",
    "def get_value_function(states, actions, vertex_values):\n",
    "    \"\"\"Perform one round of value updates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    states: ndarray\n",
    "    actions: ndarray\n",
    "    vertex_values: ndarray\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    values: ndarray\n",
    "        The updated values\n",
    "    \"\"\"\n",
    "    next_states = dynamics(states, actions)\n",
    "    rewards = reward_function(states, actions, next_states)\n",
    "    \n",
    "    expected_values = delaunay.function_values_at(next_states,\n",
    "                                                  vertex_values=vertex_values)\n",
    "    \n",
    "    # Perform value update\n",
    "    values = rewards + gamma * expected_values\n",
    "    \n",
    "    # Adapt values of terminal states\n",
    "    values[is_terminal(states)] = terminal_reward\n",
    "    \n",
    "    return values\n",
    "    \n",
    "\n",
    "def optimize_policy(states, vertex_values):\n",
    "    \"\"\"Optimize the policy for a given value function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    states: ndarray\n",
    "    vertex_values: ndarray\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    policy: ndarray\n",
    "        The optimal policy for the given value function.\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    values = np.empty((len(vertex_values), len(action_space)), dtype=np.float)\n",
    "    actions = np.empty((delaunay.nindex, 1), dtype=np.float)\n",
    "    \n",
    "    # Compute values for each action\n",
    "    for i, action in enumerate(action_space):\n",
    "        actions[:] = action\n",
    "        values[:, i] = get_value_function(states, actions, vertex_values)\n",
    "    \n",
    "    # Select best one\n",
    "    return action_space[np.argmax(values, axis=1)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initial guess for values and policy\n",
    "values = np.zeros(delaunay.nindex)\n",
    "policy = np.random.choice(action_space, size=len(state_space))\n",
    "\n",
    "old_values = values.copy()\n",
    "old_policy = policy.copy()\n",
    "\n",
    "converged = False\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    values = get_value_function(state_space, policy, values)\n",
    "    policy = optimize_policy(state_space, values)\n",
    "        \n",
    "    # Compute errors\n",
    "    value_change = np.max(np.abs(old_values - values))\n",
    "    policy_converged = np.all(old_policy == policy)\n",
    "    \n",
    "    # Break if converged\n",
    "    if value_change <= 1e-2:# and policy_converged:\n",
    "        converged = True\n",
    "        break\n",
    "    else:\n",
    "        old_values[:] = values\n",
    "        old_policy[:] = policy\n",
    "\n",
    "if converged:\n",
    "    print('converged after {} iterations. \\nerror: {}, \\npolicy: {}'.format(i + 1, value_change, policy_converged))\n",
    "else:\n",
    "    print('didnt converge, error: {} and policy: {}'.format(value_change, policy_converged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_triangulation(values)\n",
    "plt.show()\n",
    "\n",
    "plot_triangulation(values, three_dimensional=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_triangulation(policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states = np.zeros((1000, 2), dtype=np.float)\n",
    "states[0, 0] = -0.5\n",
    "\n",
    "for i in range(len(states) - 1):\n",
    "    # interpolate action\n",
    "    action = delaunay.function_values_at(states[[i], :], vertex_values=policy)\n",
    "    action = action_space[np.argmin(np.abs(action - action_space))]\n",
    "\n",
    "    states[i+1, :] = dynamics(states[i, :], action)\n",
    "    if states[i+1, 0] >= 0.6:\n",
    "        states[i+1:, :] = states[i+1]\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = plot_triangulation(values)\n",
    "ax.plot(states[:,0], states[:, 1], lw=3, color='k')\n",
    "\n",
    "ax.set_xlabel('pos')\n",
    "ax.set_ylabel('vel')\n",
    "ax.set_xlim(domain[0])\n",
    "ax.set_ylim(domain[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
