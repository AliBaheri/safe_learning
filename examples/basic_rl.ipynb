{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "import safe_learning\n",
    "import plotting\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_triangulation(values, axis=None, three_dimensional=False, zlabel=None, **kwargs):\n",
    "    \"\"\"Plot a triangulation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    values: ndarray\n",
    "    axis: optional\n",
    "    three_dimensional: bool, optional\n",
    "        Whether to plot 3D\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    axis:\n",
    "        The axis on which we plotted.\n",
    "    \"\"\"\n",
    "    if three_dimensional:\n",
    "        if axis is None:\n",
    "            axis = Axes3D(plt.figure())\n",
    "\n",
    "        # Get the simplices and plot\n",
    "        delaunay = value_function.tri\n",
    "        simplices = delaunay.simplices(np.arange(delaunay.nsimplex))\n",
    "        c = axis.plot_trisurf(state_space[:, 0], state_space[:, 1], values[:, 0],\n",
    "                            triangles=simplices.copy(),\n",
    "                            cmap='viridis', lw=0.1, **kwargs)\n",
    "        cbar = plt.colorbar(c)\n",
    "    else:\n",
    "        if axis is None:\n",
    "            axis = plt.figure().gca()\n",
    "            \n",
    "        # Some magic reshaping to go to physical coordinates\n",
    "        vals = values.reshape(n_points[0], n_points[1]).T[::-1]\n",
    "        axis = plt.imshow(vals.copy(), origin='upper',\n",
    "                        extent=domain[0] + domain[1],\n",
    "                        aspect='auto', cmap='viridis', interpolation='bilinear', **kwargs)\n",
    "        cbar = plt.colorbar(axis)\n",
    "        axis = axis.axes\n",
    "        \n",
    "    axis.set_xlabel('position')\n",
    "    axis.set_ylabel('velocity')\n",
    "    if zlabel is not None:\n",
    "        cbar.set_label(zlabel)\n",
    "        \n",
    "    return axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TODO: Handle different terminal states in a better way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = [[-1.2, 0.7], [-.07, .07]]\n",
    "n_points = [20, 20]\n",
    "\n",
    "\n",
    "discretization = safe_learning.GridWorld(domain, n_points)\n",
    "state_space = discretization.all_points\n",
    "value_function = safe_learning.Triangulation(discretization, np.zeros_like(state_space[:, 0]), project=True,\n",
    "                                             name='tri_value_function')\n",
    "\n",
    "policy = safe_learning.Triangulation(discretization, np.ones_like(state_space[:, 0]), project=True,\n",
    "                                     name='tri_policy')\n",
    "policy.__call__ = lambda x: tf.clip_by_value(policy.evaluate(x), -1., 1.)\n",
    "\n",
    "gamma = .98\n",
    "# Maximum long-term reward is 1.\n",
    "terminal_reward = 1 - gamma\n",
    "\n",
    "@safe_learning.utilities.with_scope('true_dynamics')\n",
    "def dynamics(states, actions):\n",
    "    \"\"\"Return future states of the car\"\"\"    \n",
    "    x0 = states[:, 0] + states[:, 1]\n",
    "    x1 = states[:, 1] + 0.001 * actions[:, 0] - 0.0025 * tf.cos(3 * states[:, 0])\n",
    "    \n",
    "    return tf.stack((x0, x1), axis=1)\n",
    "\n",
    "\n",
    "@safe_learning.utilities.with_scope('reward_function')\n",
    "def reward_function(states, actions):\n",
    "    zeros = tf.zeros((states.shape[0], 1), tf.float64)\n",
    "    ones = tf.ones_like(zeros)\n",
    "    return tf.where(tf.greater(states[:, 0], 0.6), terminal_reward * ones, zeros)\n",
    "\n",
    "rl = safe_learning.PolicyIteration(\n",
    "    policy,\n",
    "    dynamics,\n",
    "    reward_function,\n",
    "    value_function,\n",
    "    gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "finally:\n",
    "    session = tf.InteractiveSession()\n",
    "    session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('optimization'):\n",
    "    old_values = np.zeros_like(rl.value_function.parameters.eval())\n",
    "    old_policy = np.zeros_like(rl.policy.parameters.eval())\n",
    "    converged = False\n",
    "    action_space = np.array([[-1.], [1.]])\n",
    "\n",
    "    # value_opt = rl.value_iteration()\n",
    "    value_opt = rl.optimize_value_function()\n",
    "    policy_loss = -tf.reduce_sum(rl.future_values(rl.state_space))\n",
    "\n",
    "\n",
    "    adapt_policy = tf.train.GradientDescentOptimizer(0.1).minimize(policy_loss,\n",
    "                                                                   var_list=[rl.policy.parameters])\n",
    "\n",
    "    # Constrain the optimization\n",
    "    adapt_policy = safe_learning.utilities.add_constraint(adapt_policy,\n",
    "                                                          var_list=[rl.policy.parameters],\n",
    "                                                          bound_list=[(-1, 1)])\n",
    "\n",
    "\n",
    "    for i in range(30):\n",
    "        # Optimize value function\n",
    "        value_opt.eval()\n",
    "\n",
    "        # Optimize policy\n",
    "        rl.discrete_policy_optimization(action_space)\n",
    "    #     for _ in range(200):\n",
    "    #         session.run(adapt_policy)\n",
    "\n",
    "        # Get new parameters\n",
    "        values, policy = session.run([rl.value_function.parameters,\n",
    "                                      rl.policy.parameters])\n",
    "\n",
    "        # Compute errors\n",
    "        value_change = np.max(np.abs(old_values - values))\n",
    "        policy_change = np.max(np.abs(old_policy - policy))\n",
    "\n",
    "        # Break if converged\n",
    "        if value_change <= 1e-1 and policy_change <= 1e-1:\n",
    "            converged = True\n",
    "            break\n",
    "        else:\n",
    "            old_values = values\n",
    "            old_policy = policy\n",
    "\n",
    "\n",
    "    if converged:\n",
    "        print('converged after {} iterations. \\nerror: {}, \\npolicy: {}'\n",
    "              .format(i + 1, value_change, policy_change))\n",
    "    else:\n",
    "        print('didnt converge, error: {} and policy: {}'\n",
    "              .format(value_change, policy_change))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_triangulation(rl.value_function.parameters.eval(), zlabel='values')\n",
    "plt.show()\n",
    "\n",
    "plot_triangulation(rl.value_function.parameters.eval(), three_dimensional=True, zlabel='values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_triangulation(rl.policy.parameters.eval(), zlabel='policy', three_dimensional=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('evaluate_dynamics'):\n",
    "    states = np.zeros((1000, 2), dtype=np.float)\n",
    "    states[0, 0] = -0.5\n",
    "\n",
    "    state = tf.placeholder(tf.float64, [1, 2])\n",
    "    dynamics = rl.dynamics(state, rl.policy(state))\n",
    "\n",
    "    for i in range(len(states) - 1):\n",
    "        states[i+1, :] = dynamics.eval(feed_dict={state: states[[i], :]})\n",
    "\n",
    "        # break if terminal\n",
    "        if states[i+1, 0] >= 0.6:\n",
    "            states[i+1:] = states[i+1]\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_triangulation(rl.value_function.parameters.eval())\n",
    "ax.plot(states[:,0], states[:, 1], lw=3, color='k')\n",
    "ax.plot(np.ones(2) * 0.6, ax.get_ylim(), lw=2, color='r')\n",
    "\n",
    "ax.set_xlabel('pos')\n",
    "ax.set_ylabel('vel')\n",
    "ax.set_xlim(domain[0])\n",
    "ax.set_ylim(domain[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
