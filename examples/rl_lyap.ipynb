{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import GPy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "%matplotlib inline\n",
    "\n",
    "# Try to import safe_rl from system\n",
    "# if it fails get it from the main folder directly instead.\n",
    "import utilities\n",
    "from plotting import plot_lyapunov_1d\n",
    "\n",
    "# If library not installed, import it from '../'\n",
    "safe_learning = utilities.import_from_directory('safe_learning', '../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "\n",
    "Optimize over the policy such that the safe set does not shrink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining a discretization of the space $[-1, 1]$ with discretization constant $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x_min, x_max, discretization\n",
    "state_limits = np.array([[-1., 1.]])\n",
    "action_limits = np.array([[-1., 1.]])\n",
    "num_states = 1000\n",
    "num_actions = 101\n",
    "\n",
    "discretization = safe_learning.GridWorld(np.vstack((state_limits, action_limits)),\n",
    "                                         [num_states, num_actions])\n",
    "action_space = safe_learning.GridWorld(action_limits, num_actions).all_points\n",
    "state_space = safe_learning.GridWorld(state_limits, num_states).all_points\n",
    "\n",
    "# Discretization constant\n",
    "tau = discretization.unit_maxes.squeeze()[0]\n",
    "\n",
    "# Initial policy: All zeros\n",
    "policy = action_space[num_actions // 2] * np.ones((len(state_space), 1))\n",
    "\n",
    "print('Grid size: {0}'.format(len(state_space)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GP dynamics model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kernel = (GPy.kern.Matern32(2, lengthscale=1, active_dims=[0, 1]) *\n",
    "          GPy.kern.Linear(2, variances=[1.2, 1], ARD=True))\n",
    "# kernel = GPy.kern.Linear(2, variances=0.004**2)\n",
    "noise_var = 0.01 ** 2\n",
    "beta = 2.\n",
    "\n",
    "# Mean dynamics\n",
    "mf = GPy.core.Mapping(2, 1)\n",
    "mf.f = lambda x: x[:, [0]]\n",
    "mf.update_gradients = lambda a, b: None\n",
    "\n",
    "mean_lipschitz = 0.8\n",
    "gp_lipschitz = beta * np.sqrt(kernel.Mat32.variance) / kernel.Mat32.lengthscale * np.max(np.abs(state_limits))\n",
    "lipschitz_dynamics = mean_lipschitz + gp_lipschitz\n",
    "\n",
    "# Define one sample as the true dynamics\n",
    "# true_dynamics = safe_learning.sample_gp_function(\n",
    "#     kernel,\n",
    "#     np.vstack([state_limits, action_limits]),\n",
    "#     num_samples=20,\n",
    "#     noise_var=noise_var,\n",
    "#     mean_function=mf.f)\n",
    "\n",
    "def true_dynamics(x, u, noise=False):\n",
    "    x_next = 1.2 * x + u\n",
    "    if noise:\n",
    "        x_next += np.sqrt(noise_var) * np.random.randn(*x.shape)\n",
    "    return 1.2 * x + u\n",
    "\n",
    "# Define a GP model over the dynamics\n",
    "gp = GPy.models.GPRegression(np.array([[0, 0]]),\n",
    "                             np.array([[0]]),\n",
    "                             kernel,\n",
    "                             noise_var=noise_var,\n",
    "                             mean_function=mf)\n",
    "\n",
    "dynamics = safe_learning.GPyGaussianProcess(gp, beta=beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Lyapunov function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lyapunov_function = safe_learning.QuadraticFunction(np.array([[1]]))\n",
    "lyapunov_function = safe_learning.Triangulation(state_limits, 3, vertex_values=[1, 0, 1])\n",
    "lypschitz_lyapunov = np.max(np.abs(lyapunov_function.gradient(state_space)))\n",
    "\n",
    "lyapunov = safe_learning.LyapunovDiscrete(state_space, lyapunov_function, dynamics,\n",
    "                                          lipschitz_dynamics, lypschitz_lyapunov, tau,\n",
    "                                          initial_set=None, policy=policy)\n",
    "\n",
    "# Specify the desired accuracy\n",
    "accuracy = np.max(lyapunov.V) / 1e10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial safe set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lyapunov.initial_safe_set = np.abs(lyapunov.discretization.squeeze()) < 0.05\n",
    "\n",
    "lyapunov.update_safe_set(accuracy=accuracy)\n",
    "plot_lyapunov_1d(lyapunov, true_dynamics, legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning for the mean dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_dynamics = dynamics.to_mean_function()\n",
    "\n",
    "def reward(state, action, next_state):\n",
    "    \"\"\"The reward.\"\"\"\n",
    "    values = -(state ** 2)\n",
    "    return values\n",
    "\n",
    "def reward_derivative(state, action):\n",
    "    return -state\n",
    "\n",
    "reward_function = safe_learning.DeterministicFunction.from_callable(\n",
    "    reward, reward_derivative)\n",
    "\n",
    "value_function = safe_learning.Triangulation(state_limits, num_states, project=True)\n",
    "value_function.vertex_values = np.ones(value_function.nindex)\n",
    "\n",
    "rl = safe_learning.PolicyIteration(\n",
    "    state_space,\n",
    "    action_space,\n",
    "    mean_dynamics,\n",
    "    reward_function,\n",
    "    function_approximator=value_function,\n",
    "    gamma=0.9,\n",
    "    terminal_states=np.abs(state_space[:, 0]) <= 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the dynamics\n",
    "\n",
    "Note that the initial policy is just all zeros!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs: lyapunov\n",
    "\n",
    "def get_safe_actions(lyapunov):\n",
    "    \"\"\"Compute safe state-action pairs on a grid.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    safe : ndarray\n",
    "        Each row represents on state, each column one action.\n",
    "    \"\"\"\n",
    "    safe = np.empty((num_states, num_actions), dtype=np.bool)\n",
    "    constant_policy = np.array([0], dtype=np.float)\n",
    "    policy_array = np.broadcast_to(constant_policy, (num_states, 1))\n",
    "\n",
    "    for i, u in enumerate(action_space):\n",
    "        constant_policy[:] = u\n",
    "        safe[:, i] = lyapunov.safety_constraint(policy_array)\n",
    "        \n",
    "    return safe\n",
    "\n",
    "\n",
    "def plot_things():\n",
    "    fig, axes = plt.subplots(2, 2, gridspec_kw={'width_ratios': [30, 1]})\n",
    "\n",
    "    # Hide fake cax\n",
    "    cax, cax1 = axes[:, 1]\n",
    "    cax1.set_visible(False)\n",
    "\n",
    "    ax0, ax1 = axes[:, 0]\n",
    "    ax0.set_ylabel('action')\n",
    "    ax1.set_xlabel('state')\n",
    "    ax1.set_ylabel('$v(x)$')\n",
    "\n",
    "    ax1.set_ylim(0, np.max(lyapunov.V))\n",
    "    ax1.set_xlim(state_limits.squeeze())\n",
    "    ax0.set_xlim(state_limits.squeeze())\n",
    "    ax0.set_ylim(action_limits.squeeze())\n",
    "    ax0.set_xticks([])\n",
    "\n",
    "    # Hide x-ticks of ax0\n",
    "    plt.setp(ax0.get_xticklabels(), visible=False)\n",
    "\n",
    "    # width between cax and main axis\n",
    "    plt.subplots_adjust(wspace=.05)\n",
    "\n",
    "    # Plot the dynamics\n",
    "    mean, bound = lyapunov.dynamics(discretization.all_points)\n",
    "    img = ax0.imshow(bound.reshape(discretization.num_points).T,\n",
    "                     origin='lower',\n",
    "                     extent=discretization.limits.ravel(),\n",
    "                     aspect='auto')\n",
    "    ax0.plot(lyapunov.dynamics.gaussian_process.X[:, 0],\n",
    "             lyapunov.dynamics.gaussian_process.X[:, 1], 'x')\n",
    "    plt.colorbar(img, cax=cax)\n",
    "\n",
    "    safe = get_safe_actions(lyapunov)    \n",
    "    safe_mask = np.ma.masked_where(~safe, safe)\n",
    "    cmap = colors.ListedColormap(['white'])\n",
    "\n",
    "    # Overlay the safety feature\n",
    "    img = ax0.imshow(safe_mask.T,\n",
    "                     origin='lower',\n",
    "                     extent=discretization.limits.ravel(),\n",
    "                     alpha=0.5, cmap=cmap,\n",
    "                     aspect='auto')\n",
    "\n",
    "    lyapunov.v_dot_negative = np.any(safe, axis=1)\n",
    "    cmax = lyapunov.max_safe_levelset(accuracy)\n",
    "    is_safe = lyapunov.V <= cmax\n",
    "\n",
    "    # Plot the Lyapunov function\n",
    "    lyap_safe = np.ma.masked_where(~is_safe, lyapunov.V)\n",
    "    lyap_unsafe = np.ma.masked_where(is_safe, lyapunov.V)\n",
    "\n",
    "    # Plot lines for the boundary of the safety feature\n",
    "    x_min_safe = np.min(lyapunov.discretization[is_safe])\n",
    "    x_max_safe = np.max(lyapunov.discretization[is_safe])\n",
    "\n",
    "    ax1.plot(lyapunov.discretization, lyap_safe, 'r')\n",
    "    ax1.plot(lyapunov.discretization, lyap_unsafe, 'b')\n",
    "\n",
    "    kw_axv = {'color': 'red',\n",
    "              'alpha': 0.5}\n",
    "    ax0.axvline(x=x_min_safe, ymin=-0.2, ymax=1, clip_on=False, **kw_axv)\n",
    "    ax1.axvline(x=x_min_safe, ymin=0, ymax=1, clip_on=False, **kw_axv)\n",
    "\n",
    "    ax0.axvline(x=x_max_safe, ymin=-0.2, ymax=1, clip_on=False, **kw_axv)\n",
    "    ax1.axvline(x=x_max_safe, ymin=0, ymax=1, clip_on=False, **kw_axv)\n",
    "    \n",
    "    # Plot the current policy\n",
    "    ax0.plot(state_space, lyapunov.policy, label='policy')\n",
    "\n",
    "    ax0.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_things()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online learning\n",
    "As we sample within this initial safe set, we gain more knowledge about the system. In particular, we iteratively select the state withing the safe set, $\\mathcal{S}_n$, where the dynamics are the most uncertain (highest variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_gp():\n",
    "    \"\"\"Update the GP model based on an actively selected data point.\"\"\"\n",
    "#     lyapunov.update_safe_set(accuracy=accuracy)\n",
    "#     safe_set = lyapunov.safe_set\n",
    "    safe = get_safe_actions(lyapunov) \n",
    "    lyapunov.v_dot_negative = np.any(safe, axis=1)\n",
    "    cmax = lyapunov.max_safe_levelset(accuracy)    \n",
    "    safe[lyapunov.V > cmax, :] = False\n",
    "    \n",
    "    # Maximum uncertainty in safe set\n",
    "    idx, idu = np.where(safe)\n",
    "    safe_states = state_space[idx, :]\n",
    "    safe_actions = action_space[idu, :]\n",
    "    \n",
    "    _, dynamics_std = lyapunov.dynamics(safe_states, safe_actions)\n",
    "    \n",
    "    # Select most uncertain state-action pair\n",
    "    max_id = np.argmax(dynamics_std)\n",
    "    max_state = safe_states[[max_id], :]\n",
    "    max_input = safe_actions[[max_id], :]\n",
    "    \n",
    "    # Add newly obtained data point to the GP\n",
    "    measurement = true_dynamics(max_state, max_input, noise=True)[:, [0]]\n",
    "    a = np.hstack((max_state, max_input))\n",
    "    lyapunov.dynamics.add_data_point(a, measurement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Update the GP model a couple of times\n",
    "for i in range(25):\n",
    "    update_gp()\n",
    "    \n",
    "plot_things()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the initial policy things still look bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the new safe set\n",
    "lyapunov.update_safe_set(accuracy=accuracy)\n",
    "plot_lyapunov_1d(lyapunov, true_dynamics, legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So let's update that policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     rl.optimize_value_function()\n",
    "#     rl.update_policy(constraint=lyapunov.safety_constraint)\n",
    "\n",
    "# # Update the policy wherever safe\n",
    "# safe = lyapunov.safety_constraint(rl.policy)\n",
    "# safe &= ~lyapunov.initial_safe_set\n",
    "# lyapunov.policy[safe] = rl.policy[safe]\n",
    "\n",
    "# lyapunov.update_safe_set(accuracy=accuracy)\n",
    "# plot_lyapunov_1d(lyapunov, true_dynamics, legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_things()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl.dynamics.gradient(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = safe_learning.Triangulation(state_limits, 5)\n",
    "policy.parameters = np.zeros_like(policy.all_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Only optimize for performance when safe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fun(param, x):\n",
    "#     x = rl.state_space\n",
    "#     policy.parameters = param\n",
    "#     u = policy(x)\n",
    "    \n",
    "#     reward = rl.reward_function(x, u, x)\n",
    "#     res = np.sum(reward + rl.value_function(rl.dynamics(x, u)))\n",
    "#     return res\n",
    "    \n",
    "# def gradient(param):\n",
    "#     x = rl.state_space\n",
    "#     policy.parameters = param\n",
    "    \n",
    "#     u = policy(x)\n",
    "    \n",
    "#     v_grad = rl.value_function.gradient(rl.dynamics(x, u))\n",
    "#     d_grad = rl.dynamics.gradient(x, u)[:, 1:]\n",
    "#     p_grad = policy.parameter_derivative(x).toarray()\n",
    "    \n",
    "#     r_grad = rl.reward_function.gradient(x, u)[:, 1:]\n",
    "        \n",
    "#     gradient = (r_grad + v_grad * d_grad) * p_grad\n",
    "#     res = np.sum(gradient, axis=0)\n",
    "#     return res\n",
    "\n",
    "# def constraint(param):\n",
    "#     policy.parameters = param\n",
    "    \n",
    "#     x = lyapunov.discretization\n",
    "#     u = policy(x)\n",
    "#     prediction = lyapunov.dynamics(x, u)\n",
    "\n",
    "#     if lyapunov.uncertain_dynamics:\n",
    "#         v_dot, v_dot_error = lyapunov.v_decrease_confidence(*prediction)\n",
    "#         # Upper bound on V_dot\n",
    "#         v_dot_bound = v_dot + v_dot_error\n",
    "#     else:\n",
    "#         v_dot_bound, _ = lyapunov.v_decrease_confidence(prediction)\n",
    "\n",
    "#     return np.sum(v_dot_bound)\n",
    "    \n",
    "\n",
    "def safety_value(param):\n",
    "    \"\"\"Try to minimize the bound on Delta_V.\"\"\"\n",
    "    policy.parameters = param\n",
    "    \n",
    "    x = lyapunov.discretization\n",
    "    u = policy(x)\n",
    "    prediction = lyapunov.dynamics(x, u)\n",
    "\n",
    "    if lyapunov.uncertain_dynamics:\n",
    "        v_dot, v_dot_error = lyapunov.v_decrease_confidence(*prediction)\n",
    "        # Upper bound on V_dot\n",
    "        v_dot_bound = v_dot + v_dot_error\n",
    "    else:\n",
    "        v_dot_bound, _ = lyapunov.v_decrease_confidence(prediction)\n",
    "\n",
    "    return np.sum(v_dot_bound)\n",
    "\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimize(safety_value, policy.parameters.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyapunov.policy = policy(lyapunov.discretization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_things()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
