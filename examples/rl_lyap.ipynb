{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import GPflow\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "%matplotlib inline\n",
    "\n",
    "import safe_learning\n",
    "import plotting\n",
    "np.random.seed(0)\n",
    "\n",
    "try:\n",
    "    session.close()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "\n",
    "Optimize over the policy such that the safe set does not shrink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining a discretization of the space $[-1, 1]$ with discretization constant $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_min, x_max, discretization\\\n",
    "state_limits = np.array([[-1., 1.]])\n",
    "action_limits = np.array([[-.5, .5]])\n",
    "num_states = 1000\n",
    "num_actions = 101\n",
    "\n",
    "discretization = safe_learning.GridWorld(np.vstack((state_limits, action_limits)),\n",
    "                                         [num_states, num_actions])\n",
    "state_disc = safe_learning.GridWorld(state_limits, num_states)\n",
    "action_disc = safe_learning.GridWorld(action_limits, num_actions)\n",
    "\n",
    "# action_space = action_disc.all_points\n",
    "# state_space = state_disc.all_points\n",
    "\n",
    "# Discretization constant\n",
    "tau = np.max(state_disc.unit_maxes)\n",
    "\n",
    "# Initial policy: All zeros\n",
    "policy = safe_learning.Triangulation(action_disc, np.zeros(len(action_disc)), name='policy')\n",
    "\n",
    "# initial_policy_id = (np.abs(-0.5 * state_space.squeeze() - action_space)).argmin(axis=0)\n",
    "# initial_policy = action_space[initial_policy_id]\n",
    "\n",
    "print('Grid size: {0}'.format(len(state_disc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GP dynamics model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel = (GPflow.kernels.Matern32(2, lengthscales=1, active_dims=[0, 1]) *\n",
    "          GPflow.kernels.Linear(2, variance=[0.2, 1], ARD=True))\n",
    "\n",
    "noise_var = 0.01 ** 2\n",
    "\n",
    "# Mean dynamics\n",
    "mean_function = safe_learning.LinearSystem(([1, 0.1]), name='prior_dynamics')\n",
    "\n",
    "mean_lipschitz = 0.8\n",
    "gp_lipschitz = 0.5 # beta * np.sqrt(kernel.Mat32.variance) / kernel.Mat32.lengthscale * np.max(np.abs(state_limits))\n",
    "lipschitz_dynamics = mean_lipschitz + gp_lipschitz\n",
    "\n",
    "# Define one sample as the true dynamics\n",
    "# true_dynamics = safe_learning.sample_gp_function(\n",
    "#     kernel,\n",
    "#     np.vstack([state_limits, action_limits]),\n",
    "#     num_samples=20,\n",
    "#     noise_var=noise_var,\n",
    "#     mean_function=mf.f)\n",
    "\n",
    "a = 1.2\n",
    "b = 1.\n",
    "q = 1.\n",
    "r = 0.5\n",
    "\n",
    "true_dynamics = safe_learning.LinearSystem((a, b), name='true_dynamics')\n",
    "\n",
    "# Define a GP model over the dynamics\n",
    "gp = GPflow.gpr.GPR(np.empty((0, 2), dtype=safe_learning.config.np_dtype),\n",
    "                    np.empty((0, 1), dtype=safe_learning.config.np_dtype),\n",
    "                    kernel,\n",
    "                    mean_function=mean_function)\n",
    "gp.likelihood.variance = noise_var\n",
    "\n",
    "dynamics = safe_learning.GaussianProcess(gp, name='gp_dynamics')\n",
    "\n",
    "k_opt, s_opt = safe_learning.utilities.dlqr(a, b, q, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Lyapunov function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lyapunov_function = safe_learning.QuadraticFunction(np.array([[1]]))\n",
    "lyapfun_disc = safe_learning.GridWorld(state_limits, 3)\n",
    "lyapunov_function = safe_learning.Triangulation(lyapfun_disc, vertex_values=[1, 0, 1],\n",
    "                                                name='lyapunov_function')\n",
    "\n",
    "# lipschitz_lyapunov = np.max(np.abs(lyapunov_function.gradient(state_space)))\n",
    "lipschitz_lyapunov = 1.\n",
    "\n",
    "lyapunov = safe_learning.Lyapunov(state_disc, lyapunov_function, dynamics,\n",
    "                                  lipschitz_dynamics, lipschitz_lyapunov, tau,\n",
    "                                  policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial safe set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyapunov.initial_safe_set = np.abs(lyapunov.discretization.all_points.squeeze()) < 0.05\n",
    "\n",
    "lyapunov.update_safe_set()\n",
    "noisy_dynamics = lambda x, u, noise: true_dynamics(x, u)\n",
    "plotting.plot_lyapunov_1d(lyapunov, noisy_dynamics, legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning for the mean dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_dynamics = dynamics.to_mean_function()\n",
    "\n",
    "reward = safe_learning.QuadraticFunction(linalg.block_diag(-q, -r), name='reward_function')\n",
    "\n",
    "rl_disc = safe_learning.GridWorld(state_limits, 21)\n",
    "value_function = safe_learning.Triangulation(rl_disc,\n",
    "                                             np.zeros(len(rl_disc)),\n",
    "                                             project=True,\n",
    "                                             name='value_function')\n",
    "\n",
    "rl = safe_learning.PolicyIteration(policy, dynamics, reward, value_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the dynamics\n",
    "\n",
    "Note that the initial policy is just all zeros!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_STORAGE = {}\n",
    "\n",
    "@safe_learning.utilities.with_scope('get_safe_set')\n",
    "def get_safe_sets(lyapunov, positive=True):\n",
    "    \n",
    "    safe_states = state_disc.index_to_state(np.where(lyapunov.safe_set))\n",
    "    safe_actions = action_disc.all_points\n",
    "    feed_dict = lyapunov.feed_dict\n",
    "\n",
    "    state_actions = np.column_stack([arr.ravel() for arr in\n",
    "                                     np.meshgrid(safe_states, safe_actions, indexing='ij')])\n",
    "    safe_set = lyapunov.safe_set.reshape(state_disc.num_points)\n",
    "    \n",
    "    c_max = np.max(lyapunov.values[lyapunov.safe_set])\n",
    "    \n",
    "    storage = safe_learning.utilities.get_storage(_STORAGE, index=lyapunov)\n",
    "    \n",
    "    if storage is None:\n",
    "        tf_state_actions = tf.placeholder(safe_learning.config.dtype,\n",
    "                                          shape=[None, state_actions.shape[1]])\n",
    "        tf_c_max = tf.placeholder(safe_learning.config.dtype, shape=())\n",
    "    \n",
    "        mean, bound = lyapunov.dynamics(tf_state_actions)\n",
    "        values = lyapunov.lyapunov_function(mean) + lyapunov.lipschitz_lyapunov * bound\n",
    "        maps_inside = tf.less(values, tf_c_max, name='maps_inside_levelset')\n",
    "    \n",
    "        dec = lyapunov.v_decrease_bound(tf_state_actions[:, :safe_states.shape[1]],\n",
    "                                        (mean, bound))\n",
    "        decreases = tf.less(dec, lyapunov.threshold)\n",
    "        \n",
    "        storage = [('tf_c_max', tf_c_max),\n",
    "                   ('tf_state_actions', tf_state_actions),\n",
    "                   ('maps_inside', maps_inside),\n",
    "                   ('mean', mean),\n",
    "                   ('decreases', decreases)]\n",
    "        safe_learning.utilities.set_storage(_STORAGE, storage, index=lyapunov)\n",
    "    else:\n",
    "        tf_c_max, tf_state_actions, maps_inside, mean, decreases = storage.values()\n",
    "\n",
    "    # Put placeholder values inside feed_dict\n",
    "    feed_dict[tf_state_actions] = state_actions\n",
    "    feed_dict[tf_c_max] = c_max\n",
    "    \n",
    "    # Evaluate\n",
    "    maps_inside, mean, decreases = session.run([maps_inside, mean, decreases],\n",
    "                                               feed_dict=feed_dict)\n",
    "    \n",
    "    # Add the mean safe set on top\n",
    "    if not positive:\n",
    "        next_state_index = lyapunov.discretization.state_to_index(mean)\n",
    "        safe_in_expectation = lyapunov.safe_set[next_state_index]\n",
    "        maps_inside &= safe_in_expectation\n",
    "        \n",
    "    maps_inside_total = np.zeros(discretization.nindex, dtype=np.bool)\n",
    "    maps_inside_total = maps_inside_total.reshape(discretization.num_points)\n",
    "    decreases_total = np.zeros_like(maps_inside_total)\n",
    "    \n",
    "    maps_inside_total[safe_set, :] = maps_inside.reshape(len(safe_states), len(safe_actions))\n",
    "    decreases_total[safe_set, :] = decreases.reshape(len(safe_states), len(safe_actions))\n",
    "\n",
    "    return maps_inside_total, decreases_total\n",
    "\n",
    "\n",
    "@safe_learning.utilities.with_scope('plot_lyapunov_2d')\n",
    "def plot_things():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10), gridspec_kw={'width_ratios': [30, 1]})\n",
    "\n",
    "    # Hide fake cax\n",
    "    cax, cax1 = axes[:, 1]\n",
    "    cax1.set_visible(False)\n",
    "    cax.set_ylabel('Standard deviation $\\sigma$')\n",
    "\n",
    "    ax0, ax1 = axes[:, 0]\n",
    "    ax0.set_ylabel('action')\n",
    "    ax1.set_xlabel('state')\n",
    "    ax1.set_ylabel('$v(\\mathbf{x})$')\n",
    "\n",
    "    ax1.set_ylim(0, np.max(lyapunov.values))\n",
    "    ax1.set_xlim(state_limits.squeeze())\n",
    "    ax0.set_xlim(state_limits.squeeze())\n",
    "    ax0.set_ylim(action_limits.squeeze())\n",
    "    ax0.set_xticks([])\n",
    "\n",
    "    # Hide x-ticks of ax0\n",
    "    plt.setp(ax0.get_xticklabels(), visible=False)\n",
    "\n",
    "    # width between cax and main axis\n",
    "    plt.subplots_adjust(wspace=.05)\n",
    "    feed_dict = lyapunov.feed_dict\n",
    "        \n",
    "    # Plot the dynamics\n",
    "    states = lyapunov.discretization.all_points\n",
    "    state_actions = discretization.all_points\n",
    "    \n",
    "    storage = safe_learning.utilities.get_storage(_STORAGE, index=lyapunov)\n",
    "    if storage is None:\n",
    "        actions = lyapunov.policy(states)\n",
    "        next_states = lyapunov.dynamics(state_actions)\n",
    "        \n",
    "        storage = [('actions', actions),\n",
    "                   ('next_states',next_states)]\n",
    "        \n",
    "        safe_learning.utilities.set_storage(_STORAGE, storage, index=lyapunov)\n",
    "    else:\n",
    "        actions, next_states = storage.values()\n",
    "    \n",
    "    mean, bound = session.run(next_states, feed_dict=feed_dict)\n",
    "    \n",
    "    # Show the GP variance\n",
    "    img = ax0.imshow(bound.reshape(discretization.num_points).T,\n",
    "                     origin='lower',\n",
    "                     extent=discretization.limits.ravel(),\n",
    "                     aspect='auto')\n",
    "    \n",
    "    # Plot the dynamics\n",
    "    ax0.plot(lyapunov.dynamics.X[:, 0],\n",
    "             lyapunov.dynamics.X[:, 1], 'x')\n",
    "    cbar = plt.colorbar(img, cax=cax)\n",
    "\n",
    "    safe, safe_expanders = get_safe_sets(lyapunov)    \n",
    "    safe = safe.reshape(discretization.num_points)\n",
    "    v_dec = safe_expanders.reshape(discretization.num_points)\n",
    "    \n",
    "    safe_mask = np.ma.masked_where(~safe, safe)\n",
    "    v_dec_mask = np.ma.masked_where(~v_dec, v_dec)\n",
    "    \n",
    "\n",
    "    # Overlay the safety feature\n",
    "    img = ax0.imshow(safe_mask.T,\n",
    "                     origin='lower',\n",
    "                     extent=discretization.limits.ravel(),\n",
    "                     alpha=0.2,\n",
    "                     cmap=colors.ListedColormap(['white']),\n",
    "                     aspect='auto',\n",
    "                     vmin=0,\n",
    "                     vmax=1)    \n",
    "    \n",
    "    # Overlay the safety feature\n",
    "    img = ax0.imshow(v_dec_mask.T,\n",
    "                     origin='lower',\n",
    "                     extent=discretization.limits.ravel(),\n",
    "                     alpha=0.5,\n",
    "                     cmap=colors.ListedColormap(['red']),\n",
    "                     aspect='auto',\n",
    "                     vmin=0,\n",
    "                     vmax=1)\n",
    "    \n",
    "    is_safe = lyapunov.safe_set\n",
    "    # Plot the Lyapunov function\n",
    "    lyap_safe = np.ma.masked_where(~is_safe, lyapunov.values)\n",
    "    lyap_unsafe = np.ma.masked_where(is_safe, lyapunov.values)\n",
    "\n",
    "    # Plot lines for the boundary of the safety feature\n",
    "    x_min_safe = np.min(states[is_safe])\n",
    "    x_max_safe = np.max(states[is_safe])\n",
    "\n",
    "    ax1.plot(states, lyap_safe, 'r')\n",
    "    ax1.plot(states, lyap_unsafe, 'b')\n",
    "\n",
    "    kw_axv = {'color': 'red',\n",
    "              'alpha': 0.5}\n",
    "    ax0.axvline(x=x_min_safe, ymin=-0.2, ymax=1, clip_on=False, **kw_axv)\n",
    "    ax1.axvline(x=x_min_safe, ymin=0, ymax=1, clip_on=False, **kw_axv)\n",
    "\n",
    "    ax0.axvline(x=x_max_safe, ymin=-0.2, ymax=1, clip_on=False, **kw_axv)\n",
    "    ax1.axvline(x=x_max_safe, ymin=0, ymax=1, clip_on=False, **kw_axv)\n",
    "    \n",
    "    # Plot the current policy\n",
    "    actions = actions.eval(feed_dict=feed_dict)\n",
    "    ax0.step(states, actions, label='safe policy', alpha=0.5)\n",
    "\n",
    "    ax0.legend()\n",
    "    plt.show()\n",
    "\n",
    "# optimize_safe_policy(lyapunov)\n",
    "lyapunov.update_safe_set()\n",
    "plot_things()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online learning\n",
    "As we sample within this initial safe set, we gain more knowledge about the system. In particular, we iteratively select the state withing the safe set, $\\mathcal{S}_n$, where the dynamics are the most uncertain (highest variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = action_disc.all_points\n",
    "action_variation = safe_learning.GridWorld(np.array(action_limits) / 10, num_actions).all_points\n",
    "\n",
    "rl_opt_value_function = rl.optimize_value_function()\n",
    "for i in range(3):\n",
    "    rl_opt_value_function.eval(feed_dict=lyapunov.feed_dict)\n",
    "    rl.discrete_policy_optimization(action_space)\n",
    "    \n",
    "\n",
    "@safe_learning.utilities.with_scope('sample_new_safe_point')\n",
    "def get_safe_samples(lyapunov, positive=False):\n",
    "    c_max = np.max(lyapunov.values[lyapunov.safe_set])\n",
    "    \n",
    "    n_states = state_disc.ndim\n",
    "    n_actions = action_disc.ndim\n",
    "\n",
    "    storage = safe_learning.utilities.get_storage(_STORAGE, index=lyapunov)\n",
    "    \n",
    "    if storage is None:\n",
    "        # Predict dynamics\n",
    "        tf_state_actions = tf.placeholder(safe_learning.config.dtype,\n",
    "                                          shape=[None, n_states + n_actions])\n",
    "        tf_safe_states = tf.placeholder(safe_learning.config.dtype,\n",
    "                                        shape=[None, n_states])\n",
    "        tf_c_max = tf.placeholder(safe_learning.config.dtype, shape=())\n",
    "\n",
    "        tf_actions = lyapunov.policy(tf_safe_states)\n",
    "        mean, bound = lyapunov.dynamics(tf_state_actions)\n",
    "        values = lyapunov.lyapunov_function(mean) + lyapunov.lipschitz_lyapunov * bound\n",
    "        maps_inside = tf.less(values, tf_c_max, name='maps_inside_levelset')\n",
    "        \n",
    "        storage = [('tf_safe_states', tf_safe_states),\n",
    "                   ('tf_actions', tf_actions),\n",
    "                   ('tf_state_actions', tf_state_actions),\n",
    "                   ('tf_c_max', tf_c_max),\n",
    "                   ('mean', mean),\n",
    "                   ('bound', bound),\n",
    "                   ('maps_inside', maps_inside)]\n",
    "        safe_learning.utilities.set_storage(_STORAGE, storage, index=lyapunov)\n",
    "    else:\n",
    "        (tf_safe_states, tf_actions, tf_state_actions,\n",
    "         tf_c_max, mean, bound, maps_inside) = storage.values()\n",
    "        \n",
    "        \n",
    "    # Take all the states inside the level set and merge with all actions\n",
    "    safe_states = state_disc.index_to_state(np.where(lyapunov.safe_set))\n",
    "    \n",
    "    feed_dict = lyapunov.feed_dict\n",
    "    feed_dict[tf_safe_states] = safe_states\n",
    "    \n",
    "    actions = tf_actions.eval(feed_dict=feed_dict)\n",
    "    # Variation relative to current policy\n",
    "    eps = action_variation\n",
    "\n",
    "    n, m = actions.shape\n",
    "    n_variations = len(eps)\n",
    "\n",
    "    # repeat states\n",
    "    states_new = np.repeat(safe_states, len(eps), axis=0)\n",
    "\n",
    "    # generate actions\n",
    "    actions_new = np.repeat(actions, n_variations, axis=0) + np.tile(eps, (n, 1))\n",
    "\n",
    "    # within_bounds = np.any(actions_new < limits[:, 0], axis=1) | np.any(actions_new > limits[:, 1], axis=1)\n",
    "    np.clip(actions_new, action_limits[:, 0], action_limits[:, 1], out=actions_new)\n",
    "\n",
    "    # Stack to state-action vector\n",
    "    state_actions = np.column_stack((states_new, actions_new))\n",
    "    \n",
    "    \n",
    "#     state_actions = np.column_stack([arr.ravel() for arr in np.meshgrid(safe_states,\n",
    "#                                                                         safe_actions,\n",
    "#                                                                         indexing='ij')])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Update feed value\n",
    "    lyapunov.feed_dict[tf_state_actions] = state_actions\n",
    "    lyapunov.feed_dict[tf_c_max] = c_max\n",
    "    \n",
    "    # Evaluate\n",
    "    maps_inside, mean, var = session.run([maps_inside, mean, bound],\n",
    "                                         feed_dict=lyapunov.feed_dict)\n",
    "    maps_inside = maps_inside.squeeze(-1)\n",
    "    \n",
    "    # Check whether states map back to the safe set in expectation\n",
    "    if not positive:\n",
    "        next_state_index = lyapunov.discretization.state_to_index(mean)\n",
    "        safe_in_expectation = lyapunov.safe_set[next_state_index]\n",
    "        maps_inside &= safe_in_expectation\n",
    "    \n",
    "    return state_actions[maps_inside, :], var[maps_inside]\n",
    "\n",
    "\n",
    "with tf.variable_scope('add_new_measurement'):\n",
    "        tf_max_state_action = tf.placeholder(safe_learning.config.dtype,\n",
    "                                             shape=[1, state_disc.ndim + action_disc.ndim])\n",
    "        tf_measurement = true_dynamics(tf_max_state_action)\n",
    "        \n",
    "def update_gp():\n",
    "    \"\"\"Update the GP model based on an actively selected data point.\"\"\"\n",
    "    rl_opt_value_function.eval(feed_dict=lyapunov.feed_dict)\n",
    "    rl.discrete_policy_optimization(action_space)\n",
    "    \n",
    "    lyapunov.update_safe_set()\n",
    "    safe_state_actions, dynamics_std = get_safe_samples(lyapunov)\n",
    "\n",
    "    # Select most uncertain state-action pair\n",
    "    max_id = np.argmax(dynamics_std)\n",
    "    max_state_action = safe_state_actions[[max_id], :]\n",
    "\n",
    "    # Add newly obtained data point to the GP\n",
    "    lyapunov.feed_dict[tf_max_state_action] = max_state_action\n",
    "    measurement = tf_measurement.eval(feed_dict=lyapunov.feed_dict)\n",
    "\n",
    "    lyapunov.dynamics.add_data_point(max_state_action, measurement)\n",
    "    \n",
    "    # Optimize the policy for plotting\n",
    "#     optimize_safe_policy(lyapunov)\n",
    "\n",
    "update_gp()\n",
    "plot_things()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    update_gp()\n",
    "    \n",
    "lyapunov.update_safe_set()\n",
    "plot_things()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Only optimize for performance when safe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def safety_value(param):\n",
    "#     \"\"\"Try to minimize the bound on Delta_V.\"\"\"\n",
    "#     policy.parameters = param\n",
    "    \n",
    "#     x = lyapunov.discretization\n",
    "#     u = policy(x)\n",
    "#     prediction = lyapunov.dynamics(x, u)\n",
    "\n",
    "# #     if lyapunov.uncertain_dynamics:\n",
    "# #         v_dot, v_dot_error = lyapunov.v_decrease_confidence(x, prediction)\n",
    "# #         # Upper bound on V_dot\n",
    "# #         v_dot_bound = v_dot + v_dot_error\n",
    "# #     else:\n",
    "# #         v_dot_bound, _ = lyapunov.v_decrease_confidence(x, prediction)\n",
    "#     v_dot_bound = lyapunov.v_decrease_bound(x, prediction)\n",
    "\n",
    "#     return np.sum(v_dot_bound)\n",
    "\n",
    "# from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# res = minimize(safety_value, policy.parameters.copy())\n",
    "# policy.parameters = res.x\n",
    "# # print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lyapunov.policy = policy(lyapunov.discretization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot_things()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that we have the best 'safe' policy, let's find the highest performing one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rl.policy = policy(rl.state_space)\n",
    "# rl.optimize_value_function()\n",
    "# plt.plot(rl.values)\n",
    "\n",
    "# def get_safe(policy):    \n",
    "#     x = lyapunov.discretization\n",
    "#     u = policy(x)\n",
    "#     prediction = lyapunov.dynamics(x, u)\n",
    "#     v_dot_bound = lyapunov.v_decrease_bound(lyapunov.discretization, prediction)\n",
    "\n",
    "#     return v_dot_bound < lyapunov.threshold - 1e-5\n",
    "\n",
    "# # current safe states\n",
    "# safe_states = lyapunov.discretization[get_safe(policy), :]\n",
    "\n",
    "# def constraint(param):\n",
    "#     policy.parameters = param\n",
    "    \n",
    "#     x = safe_states\n",
    "#     u = policy(x)\n",
    "#     prediction = lyapunov.dynamics(x, u)        \n",
    "#     v_dot_bound = lyapunov.v_decrease_bound(x, prediction)\n",
    "\n",
    "#     return np.min(-v_dot_bound + lyapunov.threshold)\n",
    "    \n",
    "# def fun(param):\n",
    "#     x = rl.state_space\n",
    "#     policy.parameters = param\n",
    "#     u = policy(x)\n",
    "    \n",
    "#     reward = rl.reward_function(x, u, x)\n",
    "#     res = np.sum(reward + rl.value_function(rl.dynamics(x, u)))\n",
    "#     return -res\n",
    "    \n",
    "# def gradient(param):\n",
    "#     x = rl.state_space\n",
    "#     policy.parameters = param\n",
    "    \n",
    "#     u = policy(x)\n",
    "    \n",
    "#     v_grad = rl.value_function.gradient(rl.dynamics(x, u))\n",
    "#     d_grad = rl.dynamics.gradient(x, u)[:, 1:]\n",
    "#     p_grad = policy.parameter_derivative(x).toarray()\n",
    "    \n",
    "#     r_grad = rl.reward_function.gradient(x, u)[:, 1:]\n",
    "        \n",
    "#     gradient = (r_grad + v_grad * d_grad) * p_grad\n",
    "#     res = np.sum(gradient, axis=0)\n",
    "#     return -res\n",
    "\n",
    "# constraint(policy.parameters)\n",
    "# res = minimize(fun, policy.parameters, jac=gradient, constraints=({'type': 'ineq', 'fun': constraint}), options={'maxiter': 500})\n",
    "\n",
    "# policy.parameters = res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lyapunov.policy = policy(lyapunov.discretization)\n",
    "# plot_things()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x = policy.all_points\n",
    "\n",
    "# plt.plot(x, policy(x), label='current')\n",
    "# plt.plot(x, -k_opt * x, label='opt')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.arange(5)[:, None].astype(np.float)\n",
    "\n",
    "actions = 10 * np.linspace(0.1, 1, 5, dtype=np.float).reshape(5, 1)\n",
    "\n",
    "\n",
    "\n",
    "print(state_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.ascontiguousarray(state_actions).view(np.dtype((np.void, state_actions.dtype.itemsize * state_actions.shape[1])))\n",
    "_, idx = np.unique(b, return_index=True)\n",
    "\n",
    "state_actions[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.void?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
